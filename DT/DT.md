# 中山大学数据科学与计算机学院

# 移动信息工程专业-人工智能

# 本科生实验报告

课程名称：**Artificial Intelligence**										（2017-2018 学年秋季学期）

| 教学班号 | 15M3 | 专业（方向） | 移动信息工程（互联网） |
| :------: | :--: | :----------: | :--------------------: |
|          |      |              |                        |

## 一、实验题目

利用数据集，实现基于ID3、C4.5和CART的决策树，然后使用建好的决策树对给定的数据进行预测。



## 二、实验内容

### 1.算法原理

决策树是一个树结构（可以是二叉树和多叉树）。其每个非叶子节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，也就是非叶节点上的分支条件，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的属性特征，并按照其值选择输出分支，直到达到叶子节点，将叶子节点存放的类别作为决策结果。常用的决策树算法有*ID3*，*C4.5*和*CART*。它们都是采用贪心（即非回溯的）方法，自顶向下递归的分治方法构造。这几个算法选择属性划分的方法各不相同，**ID3使用的是信息增益，C4.5使用的是信息增益率，而CART使用的是Gini基尼指数**。

1. **使用信息增益作为划分属性**

   熵被用来衡量一个随机变量出现的期望值，也用来表示一个随机变量的随机程度。熵越大，一个变量的不确定性就越大（也就是可取的值很多），把它搞清楚所需要的信息量也就越大，熵是整个系统的平均消息量。 信息熵是信息论中用于度量信息量的一个概念。**一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高**。所以，信息熵也可以说是系统有序化程度的一个度量。

   * **熵（Entropy）的计算公式**：

     **数据集D的经验熵：** 
     $$
     H(D)=-\sum_{d\in D}p(d)logp(d)
     $$
     ​	**熵越大，说明系统越混乱，携带的信息就越少。熵越小，说明系统越有序，携带的信息就越多**。信息的作用就是在于消除不确定性。

     

     **特征A对数据集D的条件熵：** 
     $$
     H(D|A)=\sum_{a\in A}p(a)H(D|A=a)=\sum_{j}\frac{D_j}{D}\times H(D_j)
     $$
     ​	条件熵$H（D|A）$描述的是在A给定的条件下D的不确定性，如果条件熵越小，表示不确定性就越小，那么D就越容易确定结果。

     

     **信息增益：** 
     $$
     g(D,A)=H(D)-H(D|A)
     $$
     ​	所以使用熵减去条件熵，就得到了信息增益，他描述的不确定性的降低程度，可以用来度量两个变量的相关性。比如，在给定一个变量的条件下，另一个变量它的不确定性能够降低多少，如果不确定性降低得越多，那么它的确定性就越大，就越容易区分，两者就越相关。

   *所以在构造决策树的时候，需要优先选择信息增益大的特征点作为决策点。* 

   

2. **使用信息增益率作为划分属性**

   C4.5算法（使用信息增益率作为划分属性）继承了ID3算法（使用信息增益作为划分属性）的优点，并在以下几方面对ID3算法进行了改进：

   - 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；
   - 在树构造过程中进行剪枝；
   - 能够完成对连续属性的离散化处理；
   - 能够对不完整数据进行处理。

   **信息增益：**
   $$
   g(D,A)=H(D)-H(D|A)
   $$
   **数据集D关于特征A的信息分裂公式：**
   $$
   SplitInfo(D,A)=-\sum_{j=1}^{v}\frac{|D_j|}{|D|}\times log(\frac{|D_j|}{|D|})
   $$
   **信息增益率：**
   $$
   gRatio(D,A)=\frac{g(D,A)}{SplitInfo(D,A)}
   $$








*在构造决策树的时候，应该优先选择信息增益率大的特征点作为决策点。*



3. **使用基尼指数Gini作为划分属性** 

   基尼指数主要在CART算法中用到，随机森林中用到的属性划分标准也是它。**它度量的是数据分区或训练元组集D的不纯度，表示的是一个随机选中的样本在子集中被分错的可能性**。基尼系数的计算公式如下：
   $$
   Gini(D)=1-\sum p_i^2,其中p_i是D中第i个属性的概率
   $$
   Gini系数越大，不纯度越大，越不容易区分。假设A有v个不同的值出现在特征D中，它的二元划分有$2^v-2$种（除去自己和空集）。当考虑二元划分裂时，计算每个结果分区的不纯度加权和。比如A有两个值，则特征D被划分成D1和D2,这时Gini指数为：
   $$
   Gini_A(D)=\frac{D_1}{D}Gini{D_1}+\frac{D_2}{D}Gini(D_2)
   $$
   

   在使用Gini系数划分属性的时候，首先计算在特征A的条件下，数据集D的Gini系数：
   $$
   gini(D,A)=\sum_{j=1}^{v}p(A_j)\times gini(D_j|A=A_j)=\sum_{j=1}^{v}p(A_j)\times (1-\sum_{i=1}^n p_i^2)
   $$
   由于Gini系数用来表示不确定性的大小，并且和不确定性大小成正相关关系。所以，*在构建决策树的时候，应该选择Gini系数小的特征点作为决策点*。

   



### 2.算法流程图

#### 算法模块

~~~mermaid
graph TB

    subgraph 构造决策树
    t1[输入训练数据]-->|算法选择|t2[<b>ID3</b>]
    t1-->|算法选择|t3[<b>C4.5</b>]
    t1-->|算法选择|t4[<b>Gini</b>]
    t2-->t5[构建决策树]
    t3-->t5
    t4-->t5
    t5-->t6[决策树]
    end
    
    subgraph ID3/C4.5/Gini计算
    i1(输入数据集,数据集样本量<br>属性列集合,算法选择)-->i2(统计每一个属性中<br>每一个属性值对应<br>不同标签的数量)
    i2-->i3(根据算法选择<br>选择不同公式计算<br>信息增益/信息增益率/gini系数)
    i3-->i4(返回选中属性列)
    end
    
    subgraph 数据处理部分
    d1[输入训练/验证/测试数据集]-->d2[保存<b>训练/验证/测试数据矩阵</b>]
    end
~~~

#### 算法框架

~~~mermaid
graph LR
 	g1(数据处理部分:<br>训练数据)-->|输入训练数据|g2(构造决策树)
	g2-->|生成|g3(决策树)
	g21(数据处理部分:<br>验证数据)-->g31
 	g3-->g31(调整决策树)
 	g31-->g32(最终决策树)
 	g4(数据处理部分:<br>测试数据)-->g32
	g32-->g5(预测结果)
~~~

#### 数据处理模块

在数据处理模块中，为了节省IO时间，直接将读取到的数据保存为一个整型矩阵，但由于在构建决策树的过程中，需要不断将训练数据进行划分，所以将训练数据保存类型再进行划分：

~~~mermaid
graph LR
	g1(数据集)-->|数据处理模块|g2(数据矩阵)
	g2-->|训练数据|g3(转换函数)
	g3-->g4(保存动态数组的vector)
	g2-->|验证/测试数据|g5(矩阵数据)
~~~

#### 构造决策树模块

递归边界：

* D中的样本属于同一类别C，则将当前结点标记为C类结点。
* A为空集，或D中所有样本在A中所有特征上取值相同，将当前结点标记为叶节点，类别为D中出现最多的类
* D为空集，则将当前结点标记为叶节点，类别为父结点中出现最多的类

~~~mermaid
graph TB
	g1(输入结点,对应数据集,剩余属性列集合)-->g2(判断是否到达递归边界)
	g2-->|是|g7(将剩下的属性列作为分支链接到输入节点)
	g7-->g9(统计对应分支下的标签众数作为结果保存)
	g9-->g10(返回)
	g2-->|否|g3(利用ID3/C4.5/Gini选择属性列)
	g3-->g4(按照选定的属性列中的属性值划分输入的数据集)
	g4-->g41(在输入的剩余属性列中剔除选定属性列)
	g41-->g5(新建节点并链接到输入节点上)
	g5-->g6(新节点作为分支赋值,保存属性列以及对应值)
	g6-->|<b>新节点,对应数据集,新的属性列集合</b>|g1
~~~



### 3.关键代码（带注释）

#### 计算信息增益/信息增益率/Gini系数

* 统计输入数据集中各个属性中每个属性值对应的标签数量

~~~c++
int ID3_data[9][13][2];//维度分别对应哪一个属性、某一个属性中的哪个值、某一个值中的1/0数量
memset(ID3_data, 0, sizeof(ID3_data));
for (int i = 0; i<row; i++)
{
	label = data[i][Column - 1] == 1 ? 1 : 0;
	d_num += label;
	//第一列属性由于分支过多，做离散化处理，简单地划分为区间10-20，20-30，30-40，40-50（视为课上年龄属性）
	if (data[i][0] <= 20)//第一个属性的分叉方法
		ID3_data[0][0][label]++;
	else if (data[i][0] <= 30)
		ID3_data[0][1][label]++;
	else if (data[i][0] <= 40)
		ID3_data[0][2][label]++;
	else
		ID3_data[0][3][label]++;

	for (int j = 1; j < Column; j++)//统计每一个属性下的分叉对应的标签
		ID3_data[j][data[i][j]][label]++;
}
~~~



* 利用统计数据计算信息增益/信息增益率/Gini系数


~~~c++
	for (int i = 0; i < Column - 1; i++)
	{
		hda = 0.0;//信息增益中的条件熵
		C45 = 0.0;//信息增益率
		gini = 0.0;//gini系数
		if (col.find(i) == col.end())//如果某一列属性并不在输入的属性集合里面，说明该列已经被挑选了
			continue;
		for (int j = 0; j<rol_value[i]; j++)
		{
			tmp = ID3_data[i][j][0] + ID3_data[i][j][1];//某个属性之下某一个值的总数
          	//预先判断计算公式的分母是否为零
          	//信息增益率的计算公式
			avoid_nan_1 = tmp == 0 ? 0 : (-1.0*tmp / row*log(1.0*tmp / row));
			C45 += avoid_nan_1;
          	//gini系数的计算公式
			gini += tmp != 0 ? 1.0*tmp / row*(1.0 - (1.0*ID3_data[i][j][0] / tmp)*(1.0*ID3_data[i][j][0] / tmp) - (1.0*ID3_data[i][j][1] / tmp)*(1.0*ID3_data[i][j][1] / tmp)) : 0;
          	//信息增益中条件熵的计算公式
			avoid_nan_1 = ID3_data[i][j][0] == 0 ? 0 : (-1.0 * ID3_data[i][j][0] * log(1.0 * ID3_data[i][j][0] / tmp));
			avoid_nan_2 = ID3_data[i][j][1] == 0 ? 0 : (-1.0 * ID3_data[i][j][1] * log(1.0 * ID3_data[i][j][1] / tmp));
			//hda += 1.0 * tmp / row * (-1.0 * ID3_data[i][j][0] / tmp * log(1.0 * ID3_data[i][j][0] / tmp) - 1.0 * ID3_data[i][j][1] / tmp * log(1.0 * ID3_data[i][j][1] / tmp));
			hda += 1.0 / row * (avoid_nan_1 + avoid_nan_2);
		}
      	//选择条件熵最小的，也就是信息增益最大的
		if (hda < min_hda)
		{
			min_hda = hda;
			index_hda = i;
		}
      	//选择信息增益率最大的
		C45 = C45 == 0 ? 0.001 : C45;
		double temp = 1.0*(hd - hda) / C45;
		if (max_c45 < temp)
		{
			max_c45 = temp;
			index_c45 = i;
		}
      	//选择gini系数最小的
		if (min_gini > gini)
		{
			min_gini = gini;
			index_gini = i;
		}
	}
~~~



* 划分数据集

~~~c++
//根据属性值划分数据集
vector<vector<int*>> divide_set(vector<int*> data, int attri)
{
	vector<vector<int*>> Set;//需要返回多个数据集，用vector保存
	for (int j = 0; j < rol_value[attri]; j++)
	{
		vector<int*> tmp;
		tmp.clear();
		//按照属性列中不同属性值划分数据集
		for (int i = 0; i < data.size(); i++)
		{
			if (attri == 0)//第一列做离散化处理
			{
				if (10 * j + 10 < data[i][attri] && data[i][attri] <= 10 * j + 20)
					tmp.push_back(data[i]);
			}
			else
			{
				if (data[i][attri] == j)
					tmp.push_back(data[i]);
			}
		}
		Set.push_back(tmp);
	}
	return Set;
}
~~~



#### 构建决策树

* 到达递归边界

  ~~~c++
  if (meet_bound(data,col))
  {
  	int attri[13][2];
  	memset(attri, 0, sizeof(attri));
  	int attribute = *(col.begin());//默认选择剩余属性列中的第一个
  	int label = 0;
  	//统计剩余数据集中的标签众数
  	for (int i = 0; i < data.size(); i++)
  	{
  		label = data[i][Column - 1] == 1 ? 1 : 0;
  		if (attribute == 0)
  		{
  			if (data[i][0] <= 20)//第一个属性的分叉方法
  				attri[0][label]++;
  			else if (data[i][0] <= 30)
  				attri[1][label]++;
  			else if (data[i][0] <= 40)
  				attri[2][label]++;
  			else
  				attri[3][label]++;
  		}
  		else
  		attri[data[i][attribute]][label]++;
  	}
  	//建立分叉节点作为叶子节点
  	for (int i = 0; i < rol_value[attribute]; i++)
  	{
  		node* child = new node;
  		child->floor = p->floor + 1;
  		child->attribute = attribute;
  		if (attribute == 0)
  			child->value_attribute = attribute_0[i];
  		else
  			child->value_attribute = i;
  		child->result = attri[i][1] >= attri[i][0] ? 1 : 0;
  		child->if_end = true;
  		p->child.push_back(child);
  	}
  	return;
  }
  ~~~

* 未到达递归边界

  ~~~c++
  //通过计算ID3/C4.5/Gini，选定属性列
  int select_attri = ID3_C45_GINI(data.size(), data, col, SELECT);
  //边界判定条件中的标签全部一致，此时返回选中列为-1
  if (select_attri == -1)
  {
  	p->result = data[0][Column - 1] == 1 ? 1 : 0;
  	return;
  }
  //统计属性列中的标签众数
  int Most_1 = 0;
  for (int i = 0; i < data.size(); i++)
  {
  	Most_1 += data[i][Column - 1] == 1 ? 1 : 0;
  }
  if (Most_1 > (data.size() - Most_1))
  	p->result = 1;
  else
  	p->result = 0;

  vector<vector<int*>> data_set;
  set<int> new_col;
  new_col = col;
  new_col.erase(select_attri);//删除选中列
  data_set = divide_set(data, select_attri);//划分数据集
  //建立分支节点，递归建树
  for (int i = 0; i < rol_value[select_attri]; i++)
  {
  	if (data_set[i].size() == 0)
  		continue;
  	node* child = new node;
  	child->floor = p->floor + 1;
  	child->attribute = select_attri;
  	if (select_attri == 0)
  		child->value_attribute = 10 * i + 20;
  	else
  		child->value_attribute = i;
  	p->child.push_back(child);
  	vector<int*> tmp = data_set[i];
  	build_tree(child, data_set[i], new_col);
  }
  ~~~

  

### 4.创新点&优化

#### 运算速度以及空间利用的优化

​	考虑到在递归建树的过程中，需要不断地使用训练数据集中的子集，为了避免在使用数据集的子集的过程中不断拷贝数据从而降低运算速度以及增加空间使用，在保存的训练数据集的时候使用动态数组保存每一个样本，并且使用vector保存每一个动态数组的指针，每次使用训练集的子集，都只会使用同一份的训练数据集，不会产生过多额外的空间。



数组转换代码：

~~~c++
//将矩阵数据还换成向量保存，方便划分数据集
void convert(int data[MAX_ROW][MAX_COLUMN], vector<int*> &todata, int row)//要传值引用
{
	for (int i = 0; i < row; i++)
	{
		int* tmp = new int[row + 1];
		for (int j = 0; j < Column; j++)
		{
			tmp[j] = data[i][j];
		}
		todata.push_back(tmp);
	}
}
~~~



#### 对连续量进行离散化处理

​	在数据集中，第一列属性的值落在区间[10,50]之间，如果直接将第一列的各个值作为独立的一个分支，那么在构建树的过程中，该属性的分支会非常多，导致决策树庞大而且过拟合。所以，为了对决策树进行泛化处理，降低过拟合程度，对第一列属性值进行离散化处理。

* 离散化方法：将属性值划分区间(10,20]、(20,30]、(30,40]、(40,50]。这里并没有采用普遍的等距划分方法，原因为在实验过程中将数据集视为理论课和实验课上类似样例，第一列属性视为年龄（其实就是一开始写代码的时候受课上影响，并为了编程方便）。





#### 后剪枝

* 算法流程

  * 算法模块（在原本算法模块上增加）

    ~~~mermaid
    graph TB

        subgraph 后剪枝
        t1[输入子节点,根节点,验证数据集]-->t2[子节点是否为叶子节点]
        t2-->|是|t3[返回]
        t2-->|否|t4[遍历子节点]
        t4-->|子节点,根节点,验证数据集|t1
        t4-->|递归返回|t5[计算当前决策树准确率a]
        t5-->t6[当前子节点为叶子<br>节点时决策树准确率b]
        t6-->t7[a-b<设定容忍度?]
        t7-->|是|t8[递归剪掉以此节点为根的子树]
        t8-->t4
        t7-->|否|t4
        end
    ~~~

  * 算法框架：

~~~mermaid
graph LR
 	g1(数据处理部分:<br>训练数据)-->|输入训练数据|g2(构造决策树)
	g2-->|生成|g3(决策树)
	g21(数据处理部分:<br>验证数据)-->g31
 	g3-->g31(<b>后剪枝<b>)
 	g31-->g32(最终决策树)
 	g4(数据处理部分:<br>测试数据)-->g32
	g32-->g5(预测结果)
~~~

* 关键代码

~~~c++
//自底向上后剪枝
void hinder_cut(node* p,node* root, const int data[MAX_ROW][MAX_COLUMN], int row)
{
	if (p->child.empty())
		return;
	for (int i = 0; i < p->child.size(); i++)
	{
		hinder_cut(p->child[i], root, data, row);//先递归找到最底层节点
		double before_acc = Predict(root, data, row);//计算当前准确率
		p->child[i]->if_end = true;//假设当前节点为叶子节点
		double hinder_acc = Predict(root, data, row);//计算假设后的准确率
		if (before_acc - hinder_acc < 0.1)//阈值设置为准确率下降0.1
		{
			delete p->child[i];//删除子树
			p->child.erase(p->child.begin() + i);
		}
		else
			p->if_end = false;
	}
}
~~~



*注：在该实验中剪枝准确率容忍度为最多降低0.1*




## 三、实验结果及分析

### 1.实验结果展示示例

首先利用小数据集来展示建树过程以及建树结果，验证构建决策树的正确性。

#### 小数据集

|  0   |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   | 9(label) |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :------: |
|  35  |  2   |  3   |  5   |  1   |  1   |  0   |  3   |  0   |    1     |
|  23  |  1   |  2   |  2   |  1   |  0   |  2   |  1   |  0   |    1     |
|  30  |  2   |  3   |  3   |  1   |  1   |  2   |  2   |  0   |    1     |
|  42  |  3   |  3   |  0   |  0   |  0   |  2   |  3   |  0   |    -1    |
|  44  |  2   |  3   |  2   |  1   |  1   |  0   |  3   |  1   |    -1    |
|  26  |  1   |  3   |  1   |  1   |  1   |  0   |  2   |  0   |    -1    |
|  40  |  0   |  0   |  0   |  1   |  1   |  2   |  1   |  0   |    -1    |
|  21  |  3   |  3   |  1   |  1   |  1   |  2   |  2   |  0   |    -1    |
|  27  |  1   |  3   |  1   |  1   |  0   |  0   |  0   |  0   |    -1    |
|  35  |  2   |  3   |  4   |  1   |  0   |  0   |  3   |  0   |    1     |



#### 小数据集决策树

通过使用ID3、C4.5和Gini系数构建决策树如下：

***注：每一个节点的格式为（属性列，属性值，预测结果），如(3，0，0)表示该节点决策点属性为第三列属性，分支条件为值为 0，该点如果作为叶子节点，预测结果将为 0（也就是-1）。***

~~~mermaid
graph TB
	t0((root))-->t1((3,0,0))
	t0-->t2((3,1,0))
	t0-->t3((3,2,0))
	t0-->t4((3,3,1))
	t0-->t5((3,4,1))
	t0-->t6((3,5,1))
	t3-->t7((0,20-30,1))
	t3-->t8((0,40-50,0))
~~~

#### 计算验证建树过程

**第一层节点的计算数据：**

| 属性列  |    ID3    |   C4.5    |   Gini   |
| :--: | :-------: | :-------: | :------: |
|  0   | 0.145552  |  0.14136  | 0.373333 |
|  1   | 0.257123  | 0.200901  | 0.283333 |
|  2   | 0.143761  | 0.224967  |  0.375   |
|  3   | 0.534382  | 0.315132  |   0.1    |
|  4   | 0.0547462 | 0.168407  | 0.444444 |
|  5   | 0.0138443 | 0.0205707 | 0.466667 |
|  6   |     0     |     0     |   0.48   |
|  7   | 0.0661691 | 0.0517005 | 0.43333  |
|  8   | 0.0547462 | 0.168407  | 0.444444 |



在计算选择第一层节点上，明显第四列（3）分支数相对较多，信息增益/信息增益率/Gini系数明显优于其他列，所以第一层节点为第四列。

在选择第二层节点上，由于选定了第四列作为第一层节点，所以划分数据集之后可以明显区分，在属性值为0、1、3、4、5的时候，对应的标签都是一致的；在属性值为2的时候，只剩下两个数据样本。

（23  1  2  2  1  0  2  1  0  1）和（44  2  3  2  1  1  0  3  1  -1），除了第五和第六列，其他列都存在对应不同标签的分支。由于算法默认选择最先前的属性列，所以选中了第一列，对应分支为（20，30]和（30，40]。



### 2.评测指标展示及分析



***数据集划分方法：将原始数据集乱序之后，按照训练集：验证集 = 4：1 和  1：1 的比例进行划分。***



#### 指标展示——训练集：验证集 = 2：2

* 验证集准确率
  * 未剪枝
    |  算法  |   准确率    |   召回率    |   精准率    |    F1    |
    | :--: | :------: | :------: | :------: | :------: |
    | ID3  | 0.951654 | 0.986577 | 0.896341 | 0.939297 |
    | C4.5 | 0.951654 | 0.946309 | 0.927632 | 0.936877 |
    | CART | 0.951654 | 0.986577 | 0.896341 | 0.939297 |

  * 后剪枝
    |  算法  |   准确率    |   召回率    |   精准率    |    F1    |
    | :--: | :------: | :------: | :------: | :------: |
    | ID3  | 0.826972 | 0.610738 | 0.90099  |  0.728   |
    | C4.5 | 0.826972 | 0.590604 | 0.926316 | 0.721311 |
    | CART | 0.826972 | 0.610738 | 0.90099  |  0.728   |
    


* 叶子节点数量

  * 未剪枝

    |  算法  | 预测结果为1 | 预测结果为-1 |
    | :--: | :----: | :-----: |
    | ID3  |  113   |   101   |
    | C4.5 |   93   |   101   |
    | CART |  113   |   101   |

  * 后剪枝

    |  算法  | 预测结果为1 | 预测结果为-1 |
    | :--: | :----: | :-----: |
    | ID3  |   68   |   53    |
    | C4.5 |   59   |   52    |
    | CART |   68   |   53    |




#### 指标展示——训练集：验证集 = 4：1


- 验证集准确率

  * 未剪枝

    |  算法  |   准确率    |   召回率    |   精准率    |    F1    |
    | :--: | :------: | :------: | :------: | :------: |
    | ID3  | 0.609137 | 0.521739 |   0.45   | 0.483221 |
    | C4.5 | 0.634518 | 0.637681 | 0.483516 |   0.55   |
    | CART | 0.598985 | 0.507246 |  0.4375  | 0.469766 |

  * 后剪枝

    |  算法  |   准确率    |   召回率    |   精准率    |    F1    |
    | :--: | :------: | :------: | :------: | :------: |
    | ID3  | 0.639594 | 0.275362 |  0.475   | 0.348624 |
    | C4.5 | 0.634518 | 0.289855 | 0.465116 | 0.357143 |
    | CART | 0.639594 | 0.275362 |  0.475   | 0.348624 |

  

- 叶子节点数量

  - 未剪枝

    |  算法  | 预测结果为1 | 预测结果为-1 |
    | :--: | :----: | :-----: |
    | ID3  |  206   |   140   |
    | C4.5 |  210   |   146   |
    | CART |  206   |   138   |

  - 后剪枝

    |  算法  | 预测结果为1 | 预测结果为-1 |
    | :--: | :----: | :-----: |
    | ID3  |  101   |   72    |
    | C4.5 |   92   |   78    |
    | CART |  101   |   71    |




#### 分析

在划分数据集的时候，事先充分将数据集进行乱序，然后将数据集划分成1：1或4：1的两份数据集。然后利用构建的决策树模型进行数据验证。发现构建出来的决策树，有如下的特点：

* 数据集按照1：1划分的时候，剪枝前后三个算法的准确率以及树规模差别不大，甚至一致。
* 数据集按照1：1划分的时候，剪枝后三个算法的准确率等衡量模型预测能力的数据都有大约10%的降低幅度。
* 数据集按照4：1划分的时候，剪枝前三个算法的准确率以及树规模差别较大，但是在后剪枝之后三个算法的准确率以及树规模趋向一致。
* 数据集按照4：1划分的时候，剪枝后三个算法的准确率都上升了，但是召回率和F1的数据都大幅度下降。
* 在各项指标数据当中，ID3算法构建的决策树模型和CART算法构建的决策树模型高度相似。



为了充分考察决策树的构建过程，在实验过程中将决策树选取属性列的顺序打印出来查看，发现ID3算法和CART算法在构建决策树的时候选取的决策点高度相同。加上对划分好的数据集进行分析，对上述特点进行分析：

* 在划分数据集之前，为了让训练集和验证集能够充分体现数据集中的业务逻辑和避免因为训练集有所偏向导致构建的决策树过拟合，所以对给定的数据集进行了充分的乱序。对于数据集1：1划分算法差异不大的情况，是因为在充分乱序的基础上，可以认为两份数据集在业务逻辑的体现上差异不大，也就是两份数据集差异不大，所以即使构建出来的决策树过拟合，在验证的时候也能准确对验证集的数据进行预测。而当数据集按照4：1进行划分的时候，构建出来的决策树会尽可能拟合训练集（决策树算法上过拟合问题无法避免），而验证集只占数据集的五分之一，数据量比较小，所以验证结果比先前的要低。并且，在对决策树进行剪枝之后，会一定程度上降低决策树过拟合程度，所以准确率会有所上涨。
* 针对ID3算法和CART算法构建的决策树高度相似：在构建决策树的时候，三个算法中都对第一列属性进行了离散化处理，降低了第一列属性的不确定性和提高了其纯度，所以在两个算法的计算上，第一例和第四列的计算结果显示的重要性排序都是第二和第一的，上层决策点的选择很大程度上影响了下层决策点的选择。所以两个算法计算结果高度类似。两个算法的差异性可以和C4.5算法进行比对，在构建决策树的时候，C4.5算法对决策点的选择有很大的不同。因为C4.5根据的是信息增益率，受属性列的不确定性和纯度影响更大，所以会更倾向优先选择非第一列和非第四列的属性。这也可以从侧面反映出ID3算法和CART算法对属性不确定性和纯度的容忍程度比较接近。
* 剪枝过后召回率和F1的值都大幅降低：从剪枝前后可以看到，预测结果为1的叶子节点远远多于预测结果为-1的叶子节点，说明标签为-1的数据样本比较集中，差异性比较小，所以在构建决策树的时候分叉比较小；而标签为1的数据样本比较分散，差异性比较大。在进行剪枝过后，决策树的过拟合程度较大幅度降低，在验证过程中，样本数据更倾向于较集中的节点，所以预测为1的样本会减少，导致召回率和F1降低。



## 四、思考题

1. 决策树有哪些避免过拟合的方法？

   * 剪枝：
     * 预剪枝：控制树的层数、在生成树的过程中评估节点的必要性、控制节点分叉数量
     * 后剪枝：生成完整的决策树之后，自底向上地对非叶节点进行考察，基于模型复杂度进行剪枝、基于验证数据集准确率进行剪枝
   * 使用随机森林：对训练数据集进行有放回抽样的操作，取样N次，生成N个训练数据集，用抽样后的训练数据集构建N个树形成森林进行决策。
   * 训练数据样本合理，尽可能符合和反映业务逻辑和业务场景。
   * 在构建决策树的时候，使用交叉验证的方法，有利于降低过拟合程度。

2. C4.5相比于ID3的优点是什么

   * 过拟合程度较低，准确率相对有所提高

   - 利用信息增益率选择属性列，避免了ID3用信息增益选择属性列时偏向选择分支取值多的属性列的情况；
   - 能够完成对连续属性的离散化处理；
   - 能够对不完整数据进行处理。

3. 如何用决策树来判断特征的重要性

   * 在决策树中不同层级的特征上，越靠近根节点的特征越重要。因为在构建决策树的时候，优先选择的节点是信息增益/信息增益率大的或者Gini系数小的特征，这种节点确定性高，信息量丰富。
   * 在决策树同一层级的特征上，分支节点较多的节点较重要。因为在同一层级上，特征的信息增益/信息增益率/Gini系数差别不大，分支节点较多的特征，信息相对明确。

